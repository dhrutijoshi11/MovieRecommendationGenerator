# -*- coding: utf-8 -*-
"""EDA and Bert Analysis_RT

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Alr9ebc5XXCc45ybMX0E0SSWFzxcksC4
"""

import pandas as pd
import numpy as np

# Load the CSV file
data= pd.read_csv('dataset.csv')

data

#Drop Columns which is irrelevant
data.drop(columns='Unnamed: 0', inplace=True)
data.drop(columns='Name', inplace=True)

data.info()

import nltk
nltk.download('vader_lexicon')

# Sentiment Analysis using NLTK Vader SentimentIntensityAnalyzer on IMDB data
from nltk.sentiment.vader import SentimentIntensityAnalyzer


# Initialize the SentimentIntensityAnalyzer
sia = SentimentIntensityAnalyzer()

# Function to get sentiment labels
def get_sentiment_label(score):
    if score >= 0.05:
        return 'Positive'
    elif score <= -0.05:
        return 'Negative'

# Calculate sentiment scores for each text
data['sentiment_score_imdb'] = data['User Reviews'].apply(lambda x: sia.polarity_scores(x)['compound'])

# Get sentiment labels
data['sentiment_imdb'] = data['sentiment_score_imdb'].apply(get_sentiment_label)

data

# Sentiment Analysis using NLTK Vader SentimentIntensityAnalyzer on RT data
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# Initialize the SentimentIntensityAnalyzer
sia = SentimentIntensityAnalyzer()

# Function to get sentiment labels
def get_sentiment_label(score):
    if score >= 0.05:
        return 'Positive'
    elif score <= -0.05:
        return 'Negative'

# Convert empty strings in 'RT_review' column to a chosen value
data['RT_review'].fillna('', inplace=True)

# Calculate sentiment scores for each text
data['sentiment_score_rt'] = data['RT_review'].apply(lambda x: sia.polarity_scores(x)['compound'])

# Get sentiment labels
data['sentiment_rt'] = data['sentiment_score_rt'].apply(get_sentiment_label)

data

# replace empty strings in the 'RT_review' column with the value 'No_review from rotten tomatoes'
chosen_value = 'No_review from rotten tomatoes'
data['RT_review'].replace('', chosen_value, inplace=True)

data

#Drop Columns which is irrelevant
data.drop(columns='sentiment_score_rt', inplace=True)
data.drop(columns='sentiment_score_imdb', inplace=True)

data

#count the number of missing (NaN) values
data.isna().sum()

data_cleaned = data.dropna()  # Create a new DataFrame without rows containing missing values
data_cleaned.isna().sum()

import seaborn as sns
# Convert the 'sentiment_rt' column to a categorical data type
data_cleaned['sentiment_rt'] = data_cleaned['sentiment_rt'].astype('category')

sns.set(style="darkgrid", font_scale=1.2)

sns.countplot(data=data, x='sentiment_rt')

# Convert the 'sentiment_imdb' column to a categorical data type
data_cleaned['sentiment_imdb'] = data_cleaned['sentiment_imdb'].astype('category')

sns.set(style="darkgrid", font_scale=1.2)

sns.countplot(data=data, x='sentiment_imdb')

import string
import nltk
from nltk.corpus import stopwords

# Download the stopwords dataset from NLTK
nltk.download('stopwords')

#load the set of English stopwords and the string module to get a list of punctuation characters.
stop = set(stopwords.words('english'))
punctuation = list(string.punctuation)
stop.update(punctuation)

from bs4 import BeautifulSoup
import re
def strip_html(text):
    soup = BeautifulSoup(text, "html.parser")
    return soup.get_text()

#Removing the square brackets
def remove_between_square_brackets(text):
    return re.sub('\[[^]]*\]', '', text)
# Removing URL's
def remove_between_square_brackets(text):
    return re.sub(r'http\S+', '', text)
#Removing the stopwords from text
def remove_stopwords(text):
    final_text = []
    for i in text.split():
        if i.strip().lower() not in stop and i.strip().lower().isalpha():
            final_text.append(i.strip().lower())
    return " ".join(final_text)
#Removing the noisy text
def denoise_text(text):
    text = strip_html(text)
    text = remove_between_square_brackets(text)
    text = remove_stopwords(text)
    return text
#Apply function on imdb review column
data_cleaned['User Reviews']=data_cleaned['User Reviews'].apply(denoise_text)

from bs4 import BeautifulSoup
import re
def strip_html(text):
    soup = BeautifulSoup(text, "html.parser")
    return soup.get_text()

#Removing the square brackets
def remove_between_square_brackets(text):
    return re.sub('\[[^]]*\]', '', text)
# Removing URL's
def remove_between_square_brackets(text):
    return re.sub(r'http\S+', '', text)
#Removing the stopwords from text
def remove_stopwords(text):
    final_text = []
    for i in text.split():
        if i.strip().lower() not in stop and i.strip().lower().isalpha():
            final_text.append(i.strip().lower())
    return " ".join(final_text)
#Removing the noisy text
def denoise_text(text):
    text = strip_html(text)
    text = remove_between_square_brackets(text)
    text = remove_stopwords(text)
    return text
#Apply function on rt review column
data_cleaned['RT_review']=data_cleaned['RT_review'].apply(denoise_text)

#Replaces the string labels 'Positive' with the integer 1, and 'Negative' with the integer. For IMDB data
data_cleaned.sentiment_imdb.replace("Positive" , 1 , inplace = True)
data_cleaned.sentiment_imdb.replace("Negative" , 0 , inplace = True)
data_cleaned.head()

#Replaces the string labels 'Positive' with the integer 1, and 'Negative' with the integer. For RT data
data_cleaned.sentiment_rt.replace("Positive" , 1 , inplace = True)
data_cleaned.sentiment_rt.replace("Negative" , 0 , inplace = True)
data_cleaned.head()

#Generates a word cloud visualization for the positive IMDb review
import matplotlib.pyplot as plt
from wordcloud import WordCloud

plt.figure(figsize=(20, 20))  # Positive imdb Review Text
wc = WordCloud(max_words=2000, width=1600, height=800).generate(" ".join(data_cleaned[data_cleaned.sentiment_imdb == 1]['User Reviews']))
plt.imshow(wc, interpolation='bilinear')

#Generates a word cloud visualization for the positive RT review
plt.figure(figsize=(20, 20))  # Positive rt Review Text
wc = WordCloud(max_words=2000, width=1600, height=800).generate(" ".join(data_cleaned[data_cleaned.sentiment_rt == 1]['RT_review']))
plt.imshow(wc, interpolation='bilinear')

#Generates a word cloud visualization for the negative IMDb review
plt.figure(figsize=(20, 20))  # Negative imdb Review Text
wc = WordCloud(max_words=2000, width=1600, height=800).generate(" ".join(data_cleaned[data_cleaned.sentiment_imdb == 0]['User Reviews']))
plt.imshow(wc, interpolation='bilinear')

#Generates a word cloud visualization for the negative RT review
plt.figure(figsize=(20, 20))  # Negative rt Review Text
wc = WordCloud(max_words=2000, width=1600, height=800).generate(" ".join(data_cleaned[data_cleaned.sentiment_imdb == 0]['RT_review']))
plt.imshow(wc, interpolation='bilinear')

import matplotlib.pyplot as plt
#Number of characters in texts of IMDB reviews
fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,8))
text_len=data_cleaned[data_cleaned['sentiment_imdb']==1]['User Reviews'].str.len()
ax1.hist(text_len,color='red')
ax1.set_title('Text with Good Reviews')
text_len=data_cleaned[data_cleaned['sentiment_imdb']==0]['User Reviews'].str.len()
ax2.hist(text_len,color='green')
ax2.set_title('Text with Bad Reviews')
fig.suptitle('Characters in texts of IMDB reviews')
plt.show()

import matplotlib.pyplot as plt
#Number of characters in texts of RT reviews
fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,8))
text_len=data_cleaned[data_cleaned['sentiment_rt']==1]['RT_review'].str.len()

ax1.hist(text_len,color='red')
ax1.set_title('Text with Good Reviews')
text_len=data_cleaned[data_cleaned['sentiment_rt']==0]['RT_review'].str.len()
ax2.hist(text_len,color='green')
ax2.set_title('Text with Bad Reviews')
fig.suptitle('Characters in texts of RT reviews')
plt.show()

#Words in texts of IMDB reviews
fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,8))
text_len=data_cleaned[data_cleaned['sentiment_imdb']==1]['User Reviews'].str.split().map(lambda x: len(x))
ax1.hist(text_len,color='red')
ax1.set_title('Text with Good Reviews')
text_len=data_cleaned[data_cleaned['sentiment_imdb']==0]['User Reviews'].str.split().map(lambda x: len(x))
ax2.hist(text_len,color='green')
ax2.set_title('Text with Bad Reviews')
fig.suptitle('Words in texts of IMDB reviews')
plt.show()

#Words in texts of RT reviews
fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,8))
text_len=data_cleaned[data_cleaned['sentiment_rt']==1]['RT_review'].str.split().map(lambda x: len(x))
ax1.hist(text_len,color='red')
ax1.set_title('Text with Good Reviews')
text_len=data_cleaned[data_cleaned['sentiment_rt']==0]['RT_review'].str.split().map(lambda x: len(x))
ax2.hist(text_len,color='green')
ax2.set_title('Text with Bad Reviews')
fig.suptitle('Words in texts of RT reviews')
plt.show()

#Average word length in each text of IMDB reviews
fig,(ax1,ax2)=plt.subplots(1,2,figsize=(20,10))
word=data_cleaned[data_cleaned['sentiment_imdb']==1]['User Reviews'].str.split().apply(lambda x : [len(i) for i in x])
sns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')
ax1.set_title('Text with Good Reviews')
word=data_cleaned[data_cleaned['sentiment_imdb']==0]['User Reviews'].str.split().apply(lambda x : [len(i) for i in x])
sns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')
ax2.set_title('Text with Bad Reviews')
fig.suptitle('Average word length in each text of IMDB reviews')

#Average word length in each text of RT reviews
fig,(ax1,ax2)=plt.subplots(1,2,figsize=(20,10))
word=data_cleaned[data_cleaned['sentiment_rt']==1]['RT_review'].str.split().apply(lambda x : [len(i) for i in x])
sns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')
ax1.set_title('Text with Good Reviews')
word=data_cleaned[data_cleaned['sentiment_rt']==0]['RT_review'].str.split().apply(lambda x : [len(i) for i in x])
sns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')
ax2.set_title('Text with Bad Reviews')
fig.suptitle('Average word length in each text of RT reviews')

#extract individual words from the 'User Reviews' column
def get_corpus(text):
    words = []
    for i in text:
        for j in i.split():
            words.append(j.strip())
    return words
corpus_imdb = get_corpus(data_cleaned['User Reviews'])
corpus_imdb[:5]

#extract individual words from the 'RT_review' column
def get_corpus(text):
    words = []
    for i in text:
        for j in i.split():
            words.append(j.strip())
    return words
corpus_rt = get_corpus(data_cleaned['RT_review'])
corpus_rt[:5]

#count the occurrences of each word in the corpus_imdb
from collections import Counter
counter = Counter(corpus_imdb)
most_common = counter.most_common(10)
most_common_imdb = dict(most_common)
most_common_imdb

#count the occurrences of each word in the corpus_rt
from collections import Counter
counter = Counter(corpus_rt)
most_common = counter.most_common(10)
most_common_rt = dict(most_common)
most_common_rt

#returns the n most frequent n-grams (sequences of g consecutive words) found in the corpus along with their respective frequencies.
def get_top_text_ngrams(corpus, n, g):
    vec = CountVectorizer(ngram_range=(g, g)).fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:n]

#Unigram Analysis ON imdb REVIEWS
from sklearn.feature_extraction.text import CountVectorizer
import plotly.express as px
most_common_uni = get_top_text_ngrams(data_cleaned['User Reviews'],20,1)
most_common_uni = dict(most_common_uni)
temp = pd.DataFrame(columns = ["Common_words" , 'Count'])
temp["Common_words"] = list(most_common_uni.keys())
temp["Count"] = list(most_common_uni.values())
fig = px.bar(temp, x="Count", y="Common_words", title='Commmon Words in Text of IMDB reviews', orientation='h',
             width=700, height=700,color='Common_words')
fig.show()

#Unigram Analysis ON RT REVIEWS
from sklearn.feature_extraction.text import CountVectorizer
most_common_uni = get_top_text_ngrams(data_cleaned['RT_review'],20,1)
most_common_uni = dict(most_common_uni)
temp = pd.DataFrame(columns = ["Common_words" , 'Count'])
temp["Common_words"] = list(most_common_uni.keys())
temp["Count"] = list(most_common_uni.values())
fig = px.bar(temp, x="Count", y="Common_words", title='Commmon Words in Text of RT reviews', orientation='h',
             width=700, height=700,color='Common_words')
fig.show()

#Bigram Analysis on Imdb Reviews
most_common_bi = get_top_text_ngrams(data_cleaned['User Reviews'],20,2)
most_common_bi = dict(most_common_bi)
temp = pd.DataFrame(columns = ["Common_words" , 'Count'])
temp["Common_words"] = list(most_common_bi.keys())
temp["Count"] = list(most_common_bi.values())
fig = px.bar(temp, x="Count", y="Common_words", title='Commmon Bigrams in Text of IMDB reviews', orientation='h',
             width=700, height=700,color='Common_words')
fig.show()

#Bigram Analysis on RT Reviews
most_common_bi = get_top_text_ngrams(data_cleaned['RT_review'],20,2)
most_common_bi = dict(most_common_bi)
temp = pd.DataFrame(columns = ["Common_words" , 'Count'])
temp["Common_words"] = list(most_common_bi.keys())
temp["Count"] = list(most_common_bi.values())
fig = px.bar(temp, x="Count", y="Common_words", title='Commmon Bigrams in Text of RT reviews', orientation='h',
             width=700, height=700,color='Common_words')
fig.show()

#Trigram Analysis on Imdb Reviews
most_common_tri = get_top_text_ngrams(data_cleaned['User Reviews'],20,3)
most_common_tri = dict(most_common_tri)
temp = pd.DataFrame(columns = ["Common_words" , 'Count'])
temp["Common_words"] = list(most_common_tri.keys())
temp["Count"] = list(most_common_tri.values())
fig = px.bar(temp, x="Count", y="Common_words", title='Commmon Trigrams in Text of IMDB reviews', orientation='h',
             width=700, height=700,color='Common_words')
fig.show()

#Trigram Analysis on RT Reviews
most_common_tri = get_top_text_ngrams(data_cleaned['RT_review'],20,3)
most_common_tri = dict(most_common_tri)
temp = pd.DataFrame(columns = ["Common_words" , 'Count'])
temp["Common_words"] = list(most_common_tri.keys())
temp["Count"] = list(most_common_tri.values())
fig = px.bar(temp, x="Count", y="Common_words", title='Commmon Trigrams in Text of RT reviews', orientation='h',
             width=700, height=700,color='Common_words')
fig.show()

import pandas as pd
from sklearn.model_selection import train_test_split

train, test= train_test_split(data_cleaned, test_size=0.2, random_state=42)
Xtrain, ytrain = train['RT_review'], train['sentiment_rt']
Xtest, ytest = test['RT_review'], test['sentiment_rt']
#splitting the train set into train and validation on RT data
Xtrain,Xval,ytrain,yval=train_test_split(Xtrain,ytrain,
                                             test_size=0.2,random_state=10)

pip install tokenizers

pip install transformers

import transformers
from tokenizers import BertWordPieceTokenizer
#Perform tokenization
# automatically download the vocab used during pretraining or fine-tuning a given model,use from_pretrained() method
tokenizer = transformers.AutoTokenizer.from_pretrained('distilbert-base-uncased')

pip install keras

from keras.preprocessing.text import Tokenizer

#set up the tokenizer
MAX_VOCAB_SIZE = 10000
tokenizer = Tokenizer(num_words = MAX_VOCAB_SIZE,oov_token="<oov>")
tokenizer.fit_on_texts(Xtrain)
word_index = tokenizer.word_index
#print(word_index)
V = len(word_index)
print("Vocabulary of the dataset is : ",V)

##create sequences of reviews
seq_train = tokenizer.texts_to_sequences(Xtrain)
seq_test =  tokenizer.texts_to_sequences(Xtest)

#choice of maximum length of sequences
seq_len_list = [len(i) for i in seq_train + seq_test]

#if we take the direct maximum then
max_len=max(seq_len_list)
print('Maximum length of sequence in the list: {}'.format(max_len))

# when setting the maximum length of sequence, variability around the average is used.
max_seq_len = np.mean(seq_len_list) + 2 * np.std(seq_len_list)
max_seq_len = int(max_seq_len)
print('Maximum length of the sequence when considering data only two standard deviations from average: {}'.format(max_seq_len))

perc_covered = np.sum(np.array(seq_len_list) < max_seq_len) / len(seq_len_list)*100
print('The above calculated number coveres approximately {} % of data'.format(np.round(perc_covered,2)))

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Create a tokenizer and fit on the training data
tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token="<oov>")
tokenizer.fit_on_texts(Xtrain)

# Convert text data to sequences of integers
Xtrain_enc = tokenizer.texts_to_sequences(Xtrain)
Xval_enc = tokenizer.texts_to_sequences(Xval)

# Pad the sequences to the desired length using pad_sequences from Keras
input_ids_train = pad_sequences(Xtrain_enc, maxlen=max_seq_len, padding='post', truncating='post')
input_ids_val = pad_sequences(Xval_enc, maxlen=max_seq_len, padding='post', truncating='post')


import numpy as np
ytrain = np.array(ytrain)
yval = np.array(yval)

!pip install transformers

from transformers import BertTokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Create a new tokenizer using the Hugging Face BertTokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Encode the text data using the tokenizer
Xtrain_enc = tokenizer(Xtrain.tolist(), add_special_tokens=True, padding='max_length', max_length=max_seq_len, truncation=True, return_tensors='np')
Xval_enc = tokenizer(Xval.tolist(), add_special_tokens=True, padding='max_length', max_length=max_seq_len, truncation=True, return_tensors='np')
Xtest_enc = tokenizer(Xtest.tolist(),add_special_tokens=True, padding='max_length', max_length=max_seq_len, truncation=True, return_tensors='np')
# Get the input_ids and attention_mask from the encoded outputs
input_ids_train = Xtrain_enc['input_ids']
attention_mask_train = Xtrain_enc['attention_mask']

input_ids_val = Xval_enc['input_ids']
attention_mask_val = Xval_enc['attention_mask']

input_ids_val = Xtest_enc['input_ids']
attention_mask_val = Xtest_enc['attention_mask']

import numpy as np
ytrain = np.array(ytrain)
yval = np.array(yval)

import tensorflow as tf

# Preparing our datasets
train_dataset = tf.data.Dataset.from_tensor_slices((dict(Xtrain_enc), ytrain))
val_dataset = tf.data.Dataset.from_tensor_slices((dict(Xval_enc), yval))
test_dataset = tf.data.Dataset.from_tensor_slices((dict(Xtest_enc), ytest))

#function for bert_model which builds and trains a BERT-based model for binary classification using TensorFlow/Keras.
def bert_model(train_dataset,val_dataset,transformer,max_len,epochs):
    print("----Building the model----")
    input_ids = Input(shape=(max_len,), dtype=tf.int32, name="input_ids")
    attention_mask = Input(shape=(max_len,),dtype=tf.int32,name = 'attention_mask') #attention mask
    sequence_output = transformer(input_ids,attention_mask)[0]
    cls_token = sequence_output[:, 0, :]
    x = Dense(512, activation='relu')(cls_token)
    x = Dropout(0.1)(x)
    y = Dense(1, activation='sigmoid')(x)
    model = Model(inputs=[input_ids,attention_mask], outputs=y)
    model.summary()
    model.compile(Adam(lr=2e-5), loss='binary_crossentropy', metrics=['accuracy'])
    r = model.fit(train_dataset.batch(32),batch_size = 32,
                  validation_data = val_dataset.batch(32),epochs = epochs)
                  #callbacks = callbacks
    print("Train score:", model.evaluate(train_dataset.batch(32)))
    print("Validation score:", model.evaluate(val_dataset.batch(32)))
    n_epochs = len(r.history['loss'])

    return r,model,n_epochs

transformer = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased')

#Importing Libraries
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.layers import Dense,Input, Embedding,LSTM,Dropout,Conv1D, MaxPooling1D, GlobalMaxPooling1D,Dropout,Bidirectional,Flatten,BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import plot_model

epochs = 2  # You can set this to a lower value, like 1 or 2
max_len = max_seq_len

# Call the bert_model function with the reduced number of epochs
r, model, n_epochs = bert_model(train_dataset, val_dataset, transformer, max_len, epochs)

#Evaluate Model Performance on Test set
result = model.evaluate(test_dataset.batch(32))
print(dict(zip(model.metrics_names, result)))

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
#Generate predictions for the test dataset
ypred = model.predict(test_dataset.batch(32))
ypred = ypred>0.5
#Get the confusion matrix
cf_matrix = confusion_matrix(ytest, ypred)
sns.heatmap(cf_matrix,annot = True,fmt ='g', cmap='Blues')
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.show()